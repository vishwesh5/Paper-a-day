{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Object-Contextual Representations for Semantic Segmentation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Paper Details</font>\n",
    "\n",
    "1. **Authors**: Yuhui Yuan, Xilin Chen, Jingdong Wang\n",
    "2. **Paper Link**: https://arxiv.org/pdf/1909.11065v2.pdf\n",
    "3. **Category**: Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Introduction</font>\n",
    "\n",
    "**Semantic Segmentation**: Assigning a label to each pixel in an image.\n",
    "\n",
    "**Approach**: Contextual Aggregation.\n",
    "\n",
    "**Motivation**: Class label assigned to one pixel is the category of the object that the pixel belongs to.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">What does Context mean?</font>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The context of one position typically refers to a set of positions, e.g., the surrounding pixels.\n",
    "</div>\n",
    "\n",
    "If we refer to another paper (**Context Based Object Categorization: A Critical Survey**), Contextual Features are used to represent the interaction of an object with its surroundings. It can be divided into the following 3 categories:\n",
    "1. **Semantic Context** - This focuses on object co-occurence and allows to correct label of one object without affecting the label of other objects. For example, a tree is more likely to co-occur with a plant than a whale.\n",
    "2. **Spatial Context** - This focuses on the position of objects. For example, a dog is more likely to be present above grass and below sky rater than above sky and below grass. \n",
    "3. **Scale Context** - This focuses on relative size of objects. For example, a car is relatively smaller than a truck and not the other way round.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Approach</font>\n",
    "The approach discussed in the paper consists of the following 3 steps:\n",
    "\n",
    "1. **Coarse Soft Segmentation** - This involves dividing the contextual pixels (surrounding pixels) into soft object regions. The word \"soft\" here means that our focus is NOT on carrying out accurate segmentation.\n",
    "2. **Object Region Representation** - We use the soft segmentation obtained from the above step and the pixel representation to represent each object region.\n",
    "3. **Object-Contextual Representation** (OCR) - We use the output from the above 2 steps along with Pixel-Region relation to obtain the augmented representations.\n",
    "\n",
    "<img src=\"images/paper1/image01.png\" alt=\"Pipeline of the approach\" title=\"The Pipeline of the approach\" />\n",
    "<center><b>Figure 1</b>: The pipeline of the approach discussed in the paper. <a href=\"https://arxiv.org/pdf/1909.11065v2.pdf\">Source</a></center>\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Differences</font>\n",
    "\n",
    "**OCR vs Multi-Scale Context**\n",
    "\n",
    "1. OCR differentiates contextual pixels which belong to the same class to the contextual pixels which belong to different class.\n",
    "2. Multi-Scale Context approach only differentiates pixels present at different positions.\n",
    "\n",
    "**OCR vs other Relational Context schemes**\n",
    "\n",
    "The approach discussed in the paper considers not only the object region representations but also the pixel and pixel-region relations, unlike other approaches.\n",
    "\n",
    "It should also be mentioned here that the current approach is also a relational context approach.\n",
    "\n",
    "**OCR vs Coarse-to-fine Segmentation**\n",
    "\n",
    "While \"Coarse-to-fine Segmentation\" is also followed in the current approach, the difference is the way the coarse segmentation is used. The OCR approach uses the coarse segmentation to generate a contextual representation, whereas the other approaches use it directly as an extra representation.\n",
    "\n",
    "**OCR vs Region-wise Segmentation**\n",
    "\n",
    "The region-wise segmentation first groups the pixels into **super pixels** which are then assigned a label. OCR on the other hand, uses the grouped regions to learn a better labelling for the pixels, instead of directly using them for segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Approach</font>\n",
    "\n",
    "It's now time to go into the mathematical details of the approach.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Semantic Segmentation - Problem Statement</font>\n",
    "\n",
    "Given $K$ classes, assign each pixel $p_i$ of image $I$ a label $l_i$ (which is one of the $K$ <b>unique</b> classes).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Multi-Scale Context (Optional)</b>\n",
    "\n",
    "Multi-Scale context can be represented by the following equation:\n",
    "\n",
    "$$y_i ^d = \\sum_{p_s = p_i + d \\Delta_t} K_t ^ d x_s \\tag{1}$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$y_i^d$ is the <b>output</b> representation of position $p_i$ for the $d$th dilated convolution,\n",
    "\n",
    "$d$ is the dilation rate,\n",
    "\n",
    "$t$ is the index of convolution (-1,0,1 for a 3x3 convolution),\n",
    "\n",
    "$\\Delta_t = (\\Delta_w, \\Delta_h) \\mid \\Delta_w = -1, 0, 1, \\Delta_h = -1,0,1$ for a 3x3 convolution,\n",
    "\n",
    "$x_s$ is the representation at $p_s$,\n",
    "\n",
    "$K^d$ is the kernel for $d$th dilated convolution\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Relational Context (Optional)</b>\n",
    "\n",
    "Relational context can be represented by the following equation:\n",
    "\n",
    "$$y_i = \\rho \\left(\\sum_{s \\in I} w_{is} \\delta(x_s) \\right) \\tag{2}$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$y_i$ is the <b>output</b> representation of position $p_i$,\n",
    "\n",
    "$I$ refers to the image,\n",
    "\n",
    "$w_{is}$ is the relation between $x_i$ and $x_s$,\n",
    "\n",
    "$\\delta(\\cdot)$ and $\\rho(\\cdot)$ are transform functions,\n",
    "\n",
    "$x_s$ is the representation at $p_s$\n",
    "</div>\n",
    "\n",
    "Next comes my favorite part, the formulation of the current approach.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Step 1: Soft Object Regions</font>\n",
    "\n",
    "The image $I$ is partitioned into $K$ soft object regions: {$M_1, M_2, ..., M_K$} where $M_i$ corresponds to class $i$.\n",
    "\n",
    "$M_i$ is a 2D map where each entry represents the probability of the corresponding pixel belonging to the class $i$. \n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Step 2: Object Region Representation</font>\n",
    "\n",
    "The representations of all the pixels obtained in step 1 are aggregated as follows:\n",
    "\n",
    "$$f_k = \\sum_{i \\in I} m_{ki} x_{i} \\tag{3}$$\n",
    "\n",
    "Where, $m_{ki}$ represents the **normalized** probability of the pixel $p_i$ belonging to class $k$.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Step 3: Object Contextual Representation</font>\n",
    "\n",
    "First we obtain the relation between a pixel and an object region:\n",
    "\n",
    "$$w_{ik} = \\cfrac{e^{\\kappa(x_i, f_k)}}{\\sum_{j=1}^{K} e^{\\kappa(x_i, f_k)}}\\tag{4}$$\n",
    "\n",
    "Where, \n",
    "\n",
    "$\\kappa (\\cdot)$ is a relation function,\n",
    "\n",
    "Finally, we can obtain the object contextual representation $y_i$ for pixel $p_i$ as shown below:\n",
    "\n",
    "$$y_i = \\rho \\left(\\sum_{k=1}^{K} w_{ik} \\delta(f_k) \\right) \\tag{5}$$\n",
    "\n",
    "Notice the similarity between equation (5) and equation (2). \n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Step 4: Augmented Representation</font>\n",
    "\n",
    "The final representation for pixel $p_i$ is calculated as follows:\n",
    "\n",
    "$$z_i = g([x_i^T y_i^T]^T) \\tag{6}$$\n",
    "\n",
    "$g(\\cdot)$ here is a transform function with the only purpose to join the effect of original representation $x_i$ and object contextual representation $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">References</font>\n",
    "1. Object-Contextual Representations for Semantic Segmentation - https://arxiv.org/pdf/1909.11065v2.pdf\n",
    "2. Context Based Object Categorization: A Critical Survey - https://vision.cornell.edu/se3/wp-content/uploads/2014/09/context_review08_0.pdf\n",
    "3. Jupyter Markdown - https://www.ibm.com/support/knowledgecenter/en/SSGNPV_1.1.3/dsx/markd-jupyter.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
